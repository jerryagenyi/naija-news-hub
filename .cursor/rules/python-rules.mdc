---
description: 
globs: 
alwaysApply: false
---
# Python Development Rules - Naija News Hub

## Development Philosophy
- Write efficient, maintainable scraping code
- Focus on reliability and error handling
- Implement proper rate limiting and anti-ban measures
- Follow PEP 8 style guidelines
- Respect website terms of service and robots.txt
- Use appropriate tools for different scraping scenarios

## Code Style and Structure

### File and Directory Naming
- Use kebab-case for directories
  - ✅ `scrapers/site-handlers`
  - ✅ `utils/data-processors`
  - ❌ `scrapers/siteHandlers`

### Function and Variable Naming
- Use snake_case for functions and variables
  - ✅ `process_article_content`
  - ✅ `site_config`
  - ❌ `processArticleContent`
- Use meaningful, descriptive names
```python
# Examples
def extract_article_content(url: str) -> dict:
    pass

article_count = 0
is_processing = True
```

## Web Scraping Tools and Techniques

### Tool Selection Guidelines
- Use `requests` for simple static websites
- Use `BeautifulSoup` for HTML parsing
- Use `selenium` for JavaScript-heavy websites
- Use `Crawl4AI` for complex, dynamic content
- Consider `jina` for AI-driven data structuring
- Use `firecrawl` for deep web content
- Use `agentQL` for known complex processes
- Use `multion` for exploratory tasks

### Tool-Specific Implementation
```python
# Static Content (requests + BeautifulSoup)
def scrape_static_content(url: str) -> dict:
    response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(response.content, "lxml")
    return extract_data(soup)

# Dynamic Content (Crawl4AI)
async def scrape_dynamic_content(url: str) -> dict:
    async with Crawl4AI() as crawler:
        return await crawler.scrape(
            url,
            retry_options={"max_retries": 3},
            rate_limit={"requests_per_second": 2}
        )

# Complex Processes (agentQL)
async def handle_complex_process(url: str) -> dict:
    workflow = agentQL.Workflow()
    await workflow.login(url)
    await workflow.navigate_to_target()
    return await workflow.extract_data()
```

## Data Extraction and Processing

### Text Data Handling
- Use appropriate tools based on data complexity:
  - `jina` for structured/semi-structured data
  - `firecrawl` for deep web content
  - `Crawl4AI` for dynamic content
- Implement data validation
- Handle missing data appropriately
- Use batch processing for large datasets

### Data Storage
- Store data in appropriate formats:
  - JSON for structured data
  - CSV for tabular data
  - PostgreSQL for relational data
- Implement proper data compression
- Use batch inserts for efficiency
```python
async def store_articles(articles: List[dict]) -> None:
    async with pool.acquire() as conn:
        async with conn.transaction():
            await conn.executemany(
                """
                INSERT INTO articles_data (
                    website_id, article_title, article_content
                ) VALUES ($1, $2, $3)
                """,
                [(a["website_id"], a["title"], a["content"]) for a in articles]
            )
```

## Error Handling and Retry Logic
- Implement robust error handling for:
  - Connection timeouts
  - Parsing errors
  - Dynamic content issues
  - Rate limiting responses
- Use exponential backoff for retries
- Log detailed error information
```python
async def scrape_with_retry(url: str, max_retries: int = 3) -> dict:
    for attempt in range(max_retries):
        try:
            return await scrape_article(url)
        except (TimeoutError, ConnectionError) as e:
            if attempt == max_retries - 1:
                raise
            await asyncio.sleep(2 ** attempt)  # Exponential backoff
```

## Performance Optimization
- Use appropriate parsing methods:
  - `lxml` for fast HTML/XML parsing
  - `pandas` for data manipulation
- Implement concurrent scraping with `asyncio`
- Use connection pooling for databases
- Implement caching for repeated requests
```python
async def scrape_multiple_urls(urls: List[str]) -> List[dict]:
    async with asyncio.TaskGroup() as tg:
        tasks = [tg.create_task(scrape_url(url)) for url in urls]
    return [t.result() for t in tasks]
```

## API Development
- Use FastAPI for API endpoints
- Implement proper request validation
- Use Pydantic models for data validation
- Document API endpoints with OpenAPI
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

class WebsiteInput(BaseModel):
    url: str
    name: str

@app.post("/websites")
async def add_website(website: WebsiteInput):
    try:
        site_id = await create_website(website)
        return {"id": site_id}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
```

## Monitoring and Logging
- Implement structured logging
- Use appropriate log levels
- Include relevant context in logs
- Track performance metrics
```python
logger.info(
    "Starting article extraction",
    extra={
        "website_id": website_id,
        "url": url,
        "attempt": attempt
    }
)
```

## Testing
- Write unit tests for critical functions
- Implement integration tests for workflows
- Use proper mocking for external services
- Test error handling scenarios
- Test performance under load

## Documentation
- Use docstrings for functions and classes
- Include type hints
- Document exceptions and edge cases
- Provide usage examples
```python
def extract_metadata(html: str) -> dict:
    """
    Extract article metadata from HTML content.

    Args:
        html (str): Raw HTML content

    Returns:
        dict: Extracted metadata including title, author, date

    Raises:
        ExtractionError: If required metadata is missing
    """
    pass
```