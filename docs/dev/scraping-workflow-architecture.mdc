---
description: Comprehensive documentation of the Naija News Hub scraping workflow and architecture
globs:
alwaysApply: true
---

# Naija News Hub - Scraping Workflow Architecture

**Last Updated:** April 19, 2025

## Overview

This document provides a comprehensive overview of the scraping workflow architecture used in the Naija News Hub project. It maps out the entire process from URL discovery to data storage, highlighting the key components, challenges, and solutions implemented in the system.

Naija News Hub is a news aggregation platform that collects Nigerian news articles for research and analysis. The scraped articles will be vectorized and used to train an LLM for a chat interface. This document focuses on the technical implementation of the scraping pipeline.

## Workflow Diagrams

### Basic Scraping Workflow

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│                 │     │                 │     │                 │     │                 │
│  URL Discovery  │────▶│ Article Scraping│────▶│ Data Processing │────▶│  Data Storage   │
│                 │     │                 │     │                 │     │                 │
└─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘
        │                       │                       │                       │
        ▼                       ▼                       ▼                       ▼
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│ - Sitemap       │     │ - Content       │     │ - Cleaning      │     │ - PostgreSQL    │
│ - RSS           │     │   Extraction    │     │ - Normalization │     │ - Compression   │
│ - Category Pages│     │ - Metadata      │     │ - Categorization│     │ - Relationships │
│ - Homepage Links│     │   Extraction    │     │ - Markdown      │     │ - Indexing      │
└─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘
```

### Agent-Enhanced Workflow

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│                 │     │                 │     │                 │     │                 │     │                 │     │                 │
│    Agent-1:     │────▶│    Category     │────▶│   Article URL   │────▶│    Agent-2:     │────▶│    Agent-3:     │────▶│  Data Storage   │
│ URL Discovery   │     │    Discovery    │     │    Discovery    │     │Content Extraction│     │Metadata Enhance.│     │                 │
└─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘
        │                       │                       │                       │                       │                       │
        ▼                       ▼                       ▼                       ▼                       ▼                       ▼
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│ - Base URL      │     │ - Category      │     │ - Article URLs  │     │ - Content       │     │ - Enhanced      │     │ - PostgreSQL    │
│   Processing    │     │   Extraction    │     │   from Category │     │   Extraction    │     │   Metadata      │     │ - JSON Format   │
│ - Website       │     │ - Category URL  │     │ - URL Storage   │     │ - Basic Metadata │     │ - Vectorization │     │ - Relationships │
│   Analysis      │     │   Storage       │     │   in Sitemaps   │     │   Extraction    │     │   Preparation   │     │ - Indexing      │
└─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘
```

## AI Agent Integration

The Naija News Hub scraping workflow leverages AI agents powered by large language models (LLMs) to enhance the scraping process. This approach provides greater adaptability to different website structures and improves content extraction quality.

### Multi-Site Concurrent Scraping

Our architecture supports concurrent scraping of multiple websites through Crawl4AI's `MemoryAdaptiveDispatcher` and efficient agent orchestration:

1. **Resource-Aware Concurrency**: The `MemoryAdaptiveDispatcher` automatically adjusts concurrency based on system resources, preventing memory overload.

2. **Website-Specific Agent Instances**: Each website gets its own set of agent instances to maintain context and state.

3. **Domain-Based Rate Limiting**: Requests to the same domain are rate-limited to avoid triggering anti-scraping measures.

4. **Progress Tracking**: Each website's scraping progress is tracked independently, allowing for resumption if interrupted.

5. **Prioritization**: Websites can be prioritized based on importance or update frequency.

This approach allows us to efficiently scrape multiple Nigerian news websites simultaneously while respecting system resources and website limitations.

### Agent Architecture

We implement a centralized multi-agent system with task-specific roles:

1. **Agent-1: URL Discovery** (GPT-4o-mini)
   - Analyzes website structure to identify navigation patterns
   - Discovers categories and their URLs for all websites
   - Discovers article URLs from category pages
   - Stores URLs in the database for processing by other agents
   - Uses a single agent instance to handle all websites

2. **Agent-2: Content Extraction** (GPT-4o)
   - Extracts structured content from article URLs in the sitemaps table
   - Cleans content by removing ads, navigation, social media elements
   - Formats content according to our JSON schema
   - Updates article status in the sitemaps table
   - Uses a single agent instance to process all articles

3. **Agent-3: Metadata Enhancement** (GPT-4o-mini)
   - Analyzes extracted content for additional metadata
   - Improves categorization and tagging
   - Prepares content for vectorization
   - Uses a single agent instance to process all articles

This centralized approach reduces the number of agent instances needed while maintaining specialization for each task, allowing for better resource management and simplifying the codebase.

### Agent Workflow

The agents operate in a sequential pipeline with database coordination:

1. **Category Discovery First**: The URL Discovery Agent identifies all categories and their URLs for a website and stores them in the `categories` table (linked to website_id)

2. **Article URL Discovery Second**: The URL Discovery Agent processes each category to discover article URLs and stores them in the `sitemaps` table with status "pending"

3. **Content Extraction Third**: The Content Extraction Agent pulls URLs with "pending" status from the `sitemaps` table, extracts content, stores it in the `articles` table, and updates the status in `sitemaps` to "scraped"

4. **Metadata Enhancement Last**: The Metadata Enhancement Agent processes newly scraped articles to enhance metadata, update article records, and prepare content for vectorization

This database-coordinated approach ensures comprehensive coverage while maintaining efficiency, enabling resumability, and providing robust tracking throughout the process. The shared database tables serve as coordination points between agents, allowing for better error handling and progress monitoring.

## 1. URL Discovery Process

The URL discovery process is the first step in the scraping workflow. It identifies article URLs from Nigerian news websites for later processing. This process is now enhanced with AI agent capabilities.

### Implementation

**File:** `src/web_scraper/url_discovery_agent.py`

**Key Components:**

1. **URL Discovery Agent:** AI-powered agent for discovering categories and articles
2. **Category Discovery:** Identifies all category pages on a website
3. **Article URL Discovery:** Discovers article URLs from category pages
4. **Sitemaps Storage:** Stores discovered article URLs in the sitemaps table

### Technical Implementation

```python
from agents import Agent, function_tool, Runner
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy, KeywordRelevanceScorer
from typing import List, Dict

@function_tool
async def discover_categories(base_url: str) -> List[Dict]:
    """Discover all category URLs from a website."""
    config = CrawlerRunConfig(
        deep_crawl_strategy=BestFirstCrawlingStrategy(
            max_depth=1,
            include_external=False,
            url_scorer=KeywordRelevanceScorer(
                keywords=["category", "section", "topic", "news"],
                weight=0.7
            )
        ),
        stream=True
    )

    categories = []
    async with AsyncWebCrawler() as crawler:
        async for result in await crawler.arun(base_url, config=config):
            # Use LLM to determine if this is a category page
            if is_category_page(result):
                categories.append({
                    "name": extract_category_name(result),
                    "url": result.url
                })

    # Store categories in database
    await store_categories(categories, base_url)

    return categories

@function_tool
async def discover_article_urls(category_url: str) -> List[str]:
    """Discover article URLs from a category page and store in sitemaps table."""
    config = CrawlerRunConfig(
        deep_crawl_strategy=BestFirstCrawlingStrategy(
            max_depth=2,
            include_external=False,
            url_scorer=KeywordRelevanceScorer(
                keywords=["article", "story", "news", "post"],
                weight=0.8
            )
        ),
        stream=True
    )

    article_urls = []
    async with AsyncWebCrawler() as crawler:
        async for result in await crawler.arun(category_url, config=config):
            # Use LLM to determine if this is an article page
            if is_article_page(result):
                article_urls.append(result.url)

    # Store article URLs in sitemaps table
    await store_article_urls(article_urls, category_url)

    return article_urls
```

### Agent Creation and Execution

We create and execute the URL Discovery Agent using the OpenAI Agents SDK:

```python
# Create the URL Discovery Agent
url_discovery_agent = Agent(
    name="URL Discovery Agent",
    instructions="""You are an expert at discovering URLs from Nigerian news websites.
    First, discover all category URLs from the base URL.
    Then, for each category, discover all article URLs.
    Store all discovered URLs in the database.
    Track your progress so you can resume if interrupted.""",
    model="gpt-4o-mini",
    tools=[discover_categories, discover_article_urls]
)

async def run_url_discovery(base_url: str):
    """Run the URL discovery agent on a website."""
    result = await Runner.run(url_discovery_agent, f"Discover all categories and articles from {base_url}")
    return result
```

### Multi-Site Concurrent Scraping Implementation

We implement concurrent scraping of multiple websites using Crawl4AI's `MemoryAdaptiveDispatcher` and centralized agent orchestration:

```python
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, MemoryAdaptiveDispatcher
from crawl4ai.async_dispatcher import RateLimiter, CrawlerMonitor, DisplayMode
from typing import List, Dict
from agents import Agent, Runner

# Create the centralized agents
url_discovery_agent = Agent(
    name="URL Discovery Agent",
    instructions="""You are an expert at discovering URLs from Nigerian news websites.
    First, discover all category URLs from the base URL.
    Then, for each category, discover all article URLs.
    Store all discovered URLs in the database.
    Track your progress so you can resume if interrupted.""",
    model="gpt-4o-mini",
    tools=[discover_categories, discover_article_urls]
)

content_extraction_agent = Agent(
    name="Content Extraction Agent",
    instructions="""You are an expert at extracting content from Nigerian news articles.
    Extract the title, author, publication date, content, categories, and tags.
    Clean the content by removing ads, navigation, social media buttons, etc.
    Store the extracted content in the database.
    Update the status in the sitemaps table.""",
    model="gpt-4o",
    tools=[extract_article_content]
)

metadata_enhancement_agent = Agent(
    name="Metadata Enhancement Agent",
    instructions="""You are an expert at enhancing metadata for Nigerian news articles.
    Analyze the article content to extract additional metadata.
    Enhance categorization and tagging.
    Prepare the content for vectorization.""",
    model="gpt-4o-mini",
    tools=[enhance_article_metadata]
)

async def scrape_multiple_websites(website_urls: List[str]):
    """Scrape multiple websites concurrently."""
    # Configure memory-adaptive dispatcher for resource-aware concurrency
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,  # Pause if memory exceeds 70%
        check_interval=1.0,            # Check memory every second
        max_session_permit=5,          # Maximum concurrent websites
        rate_limiter=RateLimiter(
            base_delay=(1.0, 2.0),     # Random delay between requests
            max_delay=30.0,            # Maximum backoff delay
            max_retries=3              # Maximum retry attempts
        ),
        monitor=CrawlerMonitor(
            max_visible_rows=15,
            display_mode=DisplayMode.DETAILED
        )
    )

    # Process websites in batches
    async with AsyncWebCrawler() as crawler:
        # Use streaming mode to process results as they become available
        config = CrawlerRunConfig(stream=True)

        async for result in await crawler.arun_many(
            urls=website_urls,
            config=config,
            dispatcher=dispatcher
        ):
            if result.success:
                # Process the website with the centralized agents
                await process_website(result.url)
            else:
                print(f"Failed to process {result.url}: {result.error_message}")
                # Log the failure and schedule retry if needed
                await log_scraping_failure(result.url, result.error_message)

async def process_website(website_url: str):
    """Process a website using the centralized agents."""
    # Get or create website ID
    website_id = await get_website_id(website_url)

    # Step 1: Discover categories
    discovery_result = await Runner.run(
        url_discovery_agent,
        f"Discover all categories from {website_url}"
    )

    # Get discovered category URLs from database
    categories = await get_categories_by_website_id(website_id)

    # Step 2: Discover article URLs for each category
    for category in categories:
        article_urls_result = await Runner.run(
            url_discovery_agent,
            f"Discover all article URLs from category {category['url']}"
        )

    # Step 3: Process pending articles from sitemaps table
    await process_pending_articles(website_id)

async def process_pending_articles(website_id: int, batch_size: int = 10):
    """Process pending articles from the sitemaps table."""
    while True:
        # Get a batch of pending articles
        pending_articles = await get_pending_articles(website_id, batch_size)

        if not pending_articles:
            break  # No more pending articles

        # Process each article URL to extract content
        for article in pending_articles:
            try:
                # Extract content
                content_result = await Runner.run(
                    content_extraction_agent,
                    f"Extract content from {article['url']} for website ID {website_id}"
                )

                # Get the article ID from the extraction result
                article_id = await get_article_id_from_url(article['url'])

                if article_id:
                    # Enhance metadata
                    metadata_result = await Runner.run(
                        metadata_enhancement_agent,
                        f"Enhance metadata for article {article_id}"
                    )

                    # Mark as scraped in sitemaps table
                    await update_article_status(article['id'], "scraped")
                else:
                    # Mark as failed in sitemaps table
                    await update_article_status(article['id'], "failed", "Article ID not found")
            except Exception as e:
                # Mark as failed in sitemaps table
                await update_article_status(article['id'], "failed", str(e))
```

### Crawl4AI Integration

We use Crawl4AI's `AsyncWebCrawler` for URL discovery, which provides:

1. **JavaScript Rendering:** Handles dynamic content using Playwright
2. **Asynchronous Processing:** Improves performance with async/await
3. **Configurable Crawling:** Customizable depth, rate limiting, etc.
4. **Streaming Results:** Process results as they become available
5. **Best-First Crawling:** Prioritize high-value pages using relevance scoring

### URL Validation and Storage

1. **Validation:** URLs are validated to ensure they point to actual articles
   - Check URL patterns (e.g., `/news/`, `/article/`, etc.)
   - Filter out non-article pages (e.g., category pages, tag pages)
   - Verify URL accessibility

2. **Storage:** Valid URLs are stored in the database for later processing
   - Store in `sitemaps` table with status "pending"
   - Track discovery method and timestamp

## 2. Article Extraction Process

The article extraction process retrieves and processes content from discovered URLs. This process is now enhanced with AI agent capabilities.

### Implementation

**File:** `src/web_scraper/content_extraction_agent.py`

**Key Components:**

1. **Content Extraction Agent:** AI-powered agent for extracting article content
2. **LLM Extraction Strategy:** Uses language models to extract structured content
3. **Content Cleaning:** Removes ads, navigation, and other non-article content
4. **Metadata Enhancement:** Enriches article metadata through semantic understanding

### Technical Implementation

```python
from agents import Agent, function_tool, Runner
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel
from typing import List, Dict, Optional, Any

class ArticleSchema(BaseModel):
    """Schema for article extraction."""
    title: str
    author: Optional[str] = None
    published_date: Optional[str] = None
    content: str
    categories: List[Dict[str, str]] = []
    tags: List[str] = []
    image_url: Optional[str] = None

@function_tool
async def extract_article_content(url: str, website_id: int) -> Dict[str, Any]:
    """Extract structured content from an article URL using LLM extraction."""
    # Configure LLM extraction strategy
    extraction_strategy = LLMExtractionStrategy(
        model="gpt-4o",
        schema=ArticleSchema.schema(),
        instructions="Extract the article content, removing ads, navigation, and social media elements."
    )

    config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        word_count_threshold=10,
        excluded_tags=['nav', 'footer', 'aside', 'form'],
        remove_overlay_elements=True
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url, config=config)

        if not result.success:
            return {"error": result.error_message}

        # Extract structured content using LLM
        article_data = result.extracted_content

        # Calculate additional metadata
        word_count = calculate_word_count(article_data["content"])
        reading_time = calculate_reading_time(word_count)

        # Enhance the article data
        enhanced_data = {
            "title": article_data["title"],
            "url": url,
            "content": article_data["content"],
            "author": article_data.get("author"),
            "published_date": article_data.get("published_date"),
            "image_url": article_data.get("image_url"),
            "website_id": website_id,
            "article_metadata": {
                "word_count": word_count,
                "reading_time": reading_time,
                "categories": article_data.get("categories", []),
                "tags": article_data.get("tags", []),
                "extraction_method": "llm"
            }
        }

        # Store the article in the database
        await store_article(enhanced_data)

        return enhanced_data
```

### Agent Creation and Execution

We create and execute the Content Extraction Agent using the OpenAI Agents SDK:

```python
# Create the Content Extraction Agent
content_extraction_agent = Agent(
    name="Content Extraction Agent",
    instructions="""You are an expert at extracting content from Nigerian news articles.
    Extract the title, author, publication date, content, categories, and tags.
    Clean the content by removing ads, navigation, social media buttons, etc.
    Store the extracted content in the database.""",
    model="gpt-4o",
    tools=[extract_article_content]
)

async def run_content_extraction(article_urls: List[str], website_id: int):
    """Run the content extraction agent on a list of article URLs."""
    results = []
    for url in article_urls:
        result = await Runner.run(
            content_extraction_agent,
            f"Extract content from {url} for website ID {website_id}"
        )
        results.append(result)
    return results
```

### Extraction Strategies

We implement multiple extraction strategies to handle different website structures:

1. **LLM-based Extraction:** Primary strategy using language models for content extraction
   - Implemented with `LLMExtractionStrategy`
   - Robust for unknown or complex website structures
   - Adapts to different website layouts without manual configuration

2. **CSS-based Extraction:** Fallback strategy for known website structures
   - Implemented with `JsonCssExtractionStrategy`
   - More efficient but requires manual configuration

3. **Fallback Mechanisms:** Implements fallbacks when primary strategies fail
   - Generic extraction based on common patterns
   - Heuristic-based content identification

### Content Processing

1. **HTML Cleaning:** Removes ads, navigation, social media buttons, sharing options, tag sections, and other non-article content
2. **Markdown Conversion:** Converts HTML to Markdown for better processing and LLM training
3. **Image Handling:** Extracts and processes article images
4. **Date Normalization:** Standardizes date formats
5. **Content Boundary Detection:** Identifies the start and end of the actual article content

## 3. Metadata Enhancement

Metadata enhancement is a critical part of the workflow, ensuring articles are properly categorized and enriched for research and analysis. This process is now enhanced with AI agent capabilities.

### Implementation

**File:** `src/web_scraper/metadata_enhancement_agent.py`

**Key Components:**

1. **Metadata Enhancement Agent:** AI-powered agent for enriching article metadata
2. **Category Analysis:** Analyzes and enhances article categories
3. **Tag Generation:** Generates relevant tags based on article content
4. **Vectorization Preparation:** Prepares content for vectorization

### Technical Implementation

```python
from agents import Agent, function_tool, Runner
from pydantic import BaseModel
from typing import List, Dict, Optional, Any

class EnhancedMetadata(BaseModel):
    """Schema for enhanced article metadata."""
    categories: List[Dict[str, str]]
    tags: List[str]
    summary: str
    keywords: List[str]
    sentiment: Optional[str] = None
    topics: List[str] = []
    vectorization_ready: bool = True

@function_tool
async def enhance_article_metadata(article_id: int) -> Dict[str, Any]:
    """Enhance article metadata for better categorization and vectorization."""
    # Retrieve article from database
    article = await get_article_by_id(article_id)

    if not article:
        return {"error": "Article not found"}

    # Extract existing metadata
    existing_metadata = article.get("article_metadata", {})

    # Analyze content to generate enhanced metadata
    content = article.get("content", "")
    title = article.get("title", "")

    # Generate tags if not present or insufficient
    tags = existing_metadata.get("tags", [])
    if len(tags) < 5:
        tags = generate_tags(content, title)

    # Enhance categories with additional context
    categories = existing_metadata.get("categories", [])
    enhanced_categories = enhance_categories(categories, content)

    # Generate summary for vectorization
    summary = generate_summary(content)

    # Extract keywords for improved search
    keywords = extract_keywords(content, title)

    # Identify main topics
    topics = identify_topics(content)

    # Create enhanced metadata
    enhanced_metadata = {
        "categories": enhanced_categories,
        "tags": tags,
        "summary": summary,
        "keywords": keywords,
        "topics": topics,
        "vectorization_ready": True
    }

    # Update article in database
    await update_article_metadata(article_id, enhanced_metadata)

    return enhanced_metadata
```

### Agent Creation and Execution

We create and execute the Metadata Enhancement Agent using the OpenAI Agents SDK:

```python
# Create the Metadata Enhancement Agent
metadata_enhancement_agent = Agent(
    name="Metadata Enhancement Agent",
    instructions="""You are an expert at enhancing metadata for Nigerian news articles.
    Analyze the article content to extract additional metadata.
    Enhance categorization and tagging.
    Prepare the content for vectorization.""",
    model="gpt-4o-mini",
    tools=[enhance_article_metadata]
)

async def run_metadata_enhancement(article_ids: List[int]):
    """Run the metadata enhancement agent on a list of article IDs."""
    results = []
    for article_id in article_ids:
        result = await Runner.run(
            metadata_enhancement_agent,
            f"Enhance metadata for article {article_id}"
        )
        results.append(result)
    return results
```

### Website-Specific Categories

Categories are website-specific, meaning each website has its own set of categories:

1. **Database Schema:** Categories table includes `website_id` foreign key
2. **Category Creation:** New categories are created with the website's ID
3. **Category Lookup:** Categories are looked up by name and website ID
4. **Category Enhancement:** The Metadata Enhancement Agent enriches categories with additional context

This approach ensures that categories from different websites don't conflict, even if they have the same name, while still providing rich metadata for vectorization.

## 4. Data Storage

The final step in the workflow is storing the extracted and processed data in the database.

### Implementation

**File:** `src/service_layer/article_service.py`

**Key Components:**

1. **Database Models:** ORM models for articles, websites, categories, etc.
2. **Repository Layer:** Data access methods for each entity
3. **Service Layer:** Business logic for storing and retrieving data

### Technical Implementation

```python
# Simplified implementation
def extract_and_store_article(self, url: str, website_id: int) -> Optional[Article]:
    """Extract and store an article from a URL."""
    # Extract article data
    article_data = self.article_extractor.extract_article(url, website_id)

    if not article_data:
        return None

    # Create article in database
    article = self.article_repo.create_article(article_data)

    # Add categories if available
    categories = article_data.get("article_metadata", {}).get("categories", [])
    category_urls = article_data.get("article_metadata", {}).get("category_urls", [])

    for i, category_name in enumerate(categories):
        category_url = category_urls[i] if i < len(category_urls) else None

        # Try to find existing category
        category = self.website_repo.get_category_by_name(website_id, category_name)

        if not category:
            # Create new category
            category = self.website_repo.create_category(website_id, {
                "name": category_name,
                "url": category_url or f"{base_url}/category/{category_name.lower().replace(' ', '-')}"
            })

        # Add category to article
        self.article_repo.add_article_category(article.id, category.id)

    return article
```

### Database Schema

The database schema is designed to efficiently store and retrieve scraped content, with the `sitemaps` table serving as a coordination point between agents:

```sql
-- Websites table
CREATE TABLE websites (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    url TEXT NOT NULL UNIQUE,
    description TEXT,
    logo_url TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_checked_at TIMESTAMP,
    status TEXT DEFAULT 'active'
);

-- Categories table
CREATE TABLE categories (
    id SERIAL PRIMARY KEY,
    website_id INTEGER REFERENCES websites(id),
    name TEXT NOT NULL,
    url TEXT,
    description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    page_hash TEXT,  -- For change detection
    last_checked_at TIMESTAMP,
    UNIQUE(website_id, name)
);

-- Sitemaps table (coordination between agents)
CREATE TABLE sitemaps (
    id SERIAL PRIMARY KEY,
    website_id INTEGER REFERENCES websites(id),
    category_id INTEGER REFERENCES categories(id),
    url TEXT NOT NULL,
    status TEXT DEFAULT 'pending', -- 'pending', 'scraped', 'failed', 'skipped'
    discovered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_checked_at TIMESTAMP,
    error_message TEXT,
    UNIQUE(website_id, url)
);

-- Articles table
CREATE TABLE articles (
    id SERIAL PRIMARY KEY,
    website_id INTEGER REFERENCES websites(id),
    title TEXT NOT NULL,
    url TEXT NOT NULL UNIQUE,
    author TEXT,
    published_date TIMESTAMP,
    image_url TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_checked_at TIMESTAMP,
    content TEXT
);

-- Article metadata table
CREATE TABLE article_metadata (
    id SERIAL PRIMARY KEY,
    article_id INTEGER REFERENCES articles(id),
    metadata JSONB NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Article categories junction table
CREATE TABLE article_categories (
    article_id INTEGER REFERENCES articles(id),
    category_id INTEGER REFERENCES categories(id),
    PRIMARY KEY (article_id, category_id)
);
```

The `sitemaps` table plays a crucial role in the agent workflow:

1. The URL Discovery Agent stores discovered article URLs in this table with status "pending"
2. The Content Extraction Agent pulls URLs with "pending" status, processes them, and updates their status to "scraped"
3. If extraction fails, the status is updated to "failed" with an error message
4. The table enables resumability, progress tracking, and error handling

### Data Optimization

1. **Compression:** Compresses article content to reduce storage requirements
2. **Indexing:** Creates appropriate indexes for efficient querying
3. **Archiving:** Implements data retention policies for older content

## Challenges and Solutions

### 1. Inconsistent Website Structures

**Challenge:** Nigerian news websites have varying structures, URL formats, and content organization.

**Solution:**
- Implement multiple discovery methods (sitemap, RSS, category pages)
- Use adaptive extraction strategies based on website patterns
- Implement website-specific configurations when necessary

### 2. Dynamic Content and JavaScript

**Challenge:** Many websites use JavaScript to load content dynamically.

**Solution:**
- Use Crawl4AI with Playwright for browser automation
- Configure appropriate wait times for content loading
- Implement content verification to ensure complete extraction

### 3. Anti-Scraping Measures

**Challenge:** Websites implement various anti-scraping measures.

**Solution:**
- Implement rate limiting and request throttling
- Use proxy rotation for high-volume scraping
- Respect robots.txt and implement polite scraping practices
- Use realistic user agents and browser fingerprints

### 4. Complex Data Relationships

**Challenge:** Maintaining proper relationships between websites, articles, and categories.

**Solution:**
- Implement robust database schema with appropriate foreign keys
- Use transaction management to ensure data consistency
- Implement validation before database operations
- Create utility scripts to fix data inconsistencies

### 5. Error Handling and Resilience

**Challenge:** Many potential points of failure in the scraping process.

**Solution:**
- Implement comprehensive error logging
- Use retry mechanisms with exponential backoff
- Implement circuit breakers for failing endpoints
- Create monitoring and alerting for critical failures

### 6. Resource Constraints

**Challenge:** Limited resources on standard VPS hosting.

**Solution:**
- Implement scheduled batch processing instead of continuous scraping
- Use intelligent crawling to only process changed content
- Implement content compression to reduce storage requirements
- Separate compute and storage concerns for flexibility

## Vectorization Preparation

To prepare the scraped data for vectorization and LLM training:

1. **Content Quality:** Ensure high-quality, clean content without ads or irrelevant elements
2. **Metadata Enrichment:** Include rich metadata for context (categories, publication date, source)
3. **Structured Format:** Store content in a consistent, structured format
4. **Text Normalization:** Standardize text formatting, encoding, and special characters
5. **Language Detection:** Identify and tag the language of each article
6. **Content Deduplication:** Identify and handle duplicate or near-duplicate content

## Monitoring and Management

The dashboard provides tools for monitoring and managing the scraping process:

1. **Job Management:** Start, stop, and monitor scraping jobs
2. **Error Tracking:** View and analyze scraping errors
3. **Performance Metrics:** Track scraping performance and resource usage
4. **Content Statistics:** View statistics on scraped content
5. **URL Management:** Add and manage URLs for scraping

## Conclusion

The Naija News Hub scraping workflow is designed to handle the complexities of Nigerian news websites while efficiently collecting and processing content for research and analysis. By implementing multiple discovery methods, adaptive extraction strategies, and robust error handling, the system can reliably scrape content from diverse sources.

The architecture prioritizes data quality, efficiency, and resilience, ensuring that the collected data is suitable for vectorization and LLM training. Ongoing improvements focus on enhancing extraction accuracy, optimizing resource usage, and improving monitoring capabilities.

## Ongoing Maintenance Strategy

After the initial scraping of websites, we implement an efficient maintenance strategy to keep the content up-to-date while minimizing resource usage and API costs.

### Incremental Crawling

1. **Change Detection**:
   - Store checksums or hashes of previously crawled category pages
   - Compare current page hash with stored hash to detect changes
   - Only process pages that have changed since the last crawl

2. **Optimized Crawling Frequency**:
   - **High-Activity Sites** (multiple updates per day): Every 6 hours
   - **Medium-Activity Sites** (daily updates): Once per day
   - **Low-Activity Sites** (weekly updates): Twice per week
   - Dynamically adjust based on observed update patterns

3. **Resource-Efficient Implementation**:
   - Use lightweight HEAD requests to check for changes before full crawling
   - Implement exponential backoff for sites with no changes
   - Schedule crawls during off-peak hours (e.g., midnight to 5 AM)

### Implementation

```python
async def check_for_updates(website_id: int):
    """Check if a website has new content since the last crawl."""
    website = await get_website_by_id(website_id)
    categories = await get_categories_by_website_id(website_id)

    updates_found = False

    for category in categories:
        # Get stored hash and last check time
        stored_hash = category.get("page_hash")
        last_checked = category.get("last_checked_at")

        # Check if we need to crawl based on the site's update frequency
        if not should_check_category(category, last_checked):
            continue

        # Make a lightweight HEAD request to check Last-Modified header
        headers = await make_head_request(category["url"])
        if not has_changed(headers, last_checked):
            # Update last checked time but don't crawl
            await update_category_check_time(category["id"])
            continue

        # Get the current page and calculate its hash
        page_content = await fetch_category_page(category["url"])
        current_hash = calculate_hash(page_content)

        # Compare with stored hash
        if current_hash != stored_hash:
            # Page has changed, discover new articles
            new_articles = await discover_new_articles(category, page_content)
            if new_articles:
                updates_found = True
                # Process new articles
                await process_new_articles(new_articles, website_id)

            # Update the stored hash and last checked time
            await update_category_hash(category["id"], current_hash)

    return updates_found

async def process_new_articles(article_urls: List[str], website_id: int):
    """Process newly discovered articles."""
    # Extract content using Content Extraction Agent
    for url in article_urls:
        article_data = await extract_article_content(url, website_id)

        # Store article in database
        article_id = await store_article(article_data)

        # Enhance metadata using Metadata Enhancement Agent
        enhanced_metadata = await enhance_article_metadata(article_id)

        # Vectorize article for LLM training
        await vectorize_article(article_id)
```

### Vectorization Integration

Newly discovered articles are automatically vectorized and added to the vector database for LLM training:

1. **Vectorization Process**:
   - Extract key content from the article
   - Generate embeddings using appropriate model
   - Store embeddings in vector database

2. **LLM Integration**:
   - Periodically update LLM training data with new vectors
   - Implement incremental training to incorporate new content
   - Track performance metrics to ensure quality

## Related Files

### Agent Implementation Files
- `src/web_scraper/url_discovery_agent.py`: URL Discovery Agent implementation
- `src/web_scraper/content_extraction_agent.py`: Content Extraction Agent implementation
- `src/web_scraper/metadata_enhancement_agent.py`: Metadata Enhancement Agent implementation
- `src/web_scraper/maintenance_agent.py`: Maintenance Agent implementation

### Core Implementation Files
- `src/service_layer/article_service.py`: Article service implementation
- `src/database_management/models.py`: Database models
- `src/database_management/repositories/`: Repository implementations
- `src/vectorization/article_vectorizer.py`: Article vectorization implementation

### Documentation Files
- `docs/dev/crawl4ai-integration.md`: Crawl4AI integration documentation
- `docs/dev/efficient-scraping-architecture.md`: Scraping architecture documentation
- `docs/dev/database-schema.md`: Database schema documentation
- `docs/dev/crawl4ai-with-agent-transcript.md`: Crawl4AI with agent integration documentation
- `docs/dev/maintenance-strategy.md`: Maintenance strategy documentation
