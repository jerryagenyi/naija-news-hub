---
description: Comprehensive documentation of the Naija News Hub scraping workflow and architecture
globs:
alwaysApply: true
---

# Naija News Hub - Scraping Workflow Architecture

**Last Updated:** April 19, 2025

## Overview

This document provides a comprehensive overview of the scraping workflow architecture used in the Naija News Hub project. It maps out the entire process from URL discovery to data storage, highlighting the key components, challenges, and solutions implemented in the system.

Naija News Hub is a news aggregation platform that collects Nigerian news articles for research and analysis. The scraped articles will be vectorized and used to train an LLM for a chat interface. This document focuses on the technical implementation of the scraping pipeline.

## Workflow Diagrams

### Basic Scraping Workflow

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│                 │     │                 │     │                 │     │                 │
│  URL Discovery  │────▶│ Article Scraping│────▶│ Data Processing │────▶│  Data Storage   │
│                 │     │                 │     │                 │     │                 │
└─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘
        │                       │                       │                       │
        ▼                       ▼                       ▼                       ▼
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│ - Sitemap       │     │ - Content       │     │ - Cleaning      │     │ - PostgreSQL    │
│ - RSS           │     │   Extraction    │     │ - Normalization │     │ - Compression   │
│ - Category Pages│     │ - Metadata      │     │ - Categorization│     │ - Relationships │
│ - Homepage Links│     │   Extraction    │     │ - Markdown      │     │ - Indexing      │
└─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘
```

### Agent-Enhanced Workflow

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│                 │     │                 │     │                 │     │                 │     │                 │
│  URL Discovery  │────▶│ Category        │────▶│ Article URL     │────▶│ Article Content │────▶│  Data Storage   │
│  Agent          │     │ Discovery       │     │ Discovery       │     │ Extraction      │     │                 │
└─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘
        │                       │                       │                       │                       │
        ▼                       ▼                       ▼                       ▼                       ▼
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│ - Base URL      │     │ - Category      │     │ - Article URLs  │     │ - Content       │     │ - PostgreSQL    │
│   Processing    │     │   Extraction    │     │   from Category │     │   Extraction    │     │ - JSON Format   │
│ - Website       │     │ - Category URL  │     │ - URL Storage   │     │ - Metadata      │     │ - Relationships │
│   Analysis      │     │   Storage       │     │   in Sitemaps   │     │   Enhancement   │     │ - Indexing      │
└─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘     └─────────────────┘
```

## AI Agent Integration

The Naija News Hub scraping workflow leverages AI agents powered by large language models (LLMs) to enhance the scraping process. This approach provides greater adaptability to different website structures and improves content extraction quality.

### Multi-Site Concurrent Scraping

Our architecture supports concurrent scraping of multiple websites through Crawl4AI's `MemoryAdaptiveDispatcher` and efficient agent orchestration:

1. **Resource-Aware Concurrency**: The `MemoryAdaptiveDispatcher` automatically adjusts concurrency based on system resources, preventing memory overload.

2. **Website-Specific Agent Instances**: Each website gets its own set of agent instances to maintain context and state.

3. **Domain-Based Rate Limiting**: Requests to the same domain are rate-limited to avoid triggering anti-scraping measures.

4. **Progress Tracking**: Each website's scraping progress is tracked independently, allowing for resumption if interrupted.

5. **Prioritization**: Websites can be prioritized based on importance or update frequency.

This approach allows us to efficiently scrape multiple Nigerian news websites simultaneously while respecting system resources and website limitations.

### Agent Architecture

We implement a multi-agent system with specialized roles:

1. **URL Discovery Agent** (GPT-4o-mini)
   - Analyzes website structure to identify navigation patterns
   - Determines the most effective discovery method for each website
   - Adapts to changing website structures without code modifications

2. **Content Extraction Agent** (GPT-4o)
   - Extracts structured content from article pages
   - Cleans content by removing ads, navigation, social media elements
   - Enhances metadata through semantic understanding
   - Formats content according to our JSON schema

3. **Metadata Enhancement Agent** (GPT-4o-mini)
   - Analyzes extracted content for additional metadata
   - Improves categorization and tagging
   - Prepares content for vectorization

### Agent Workflow

The agents operate in a sequential pipeline:

1. **Category Discovery First**: The URL Discovery Agent identifies all categories and their URLs for a website
2. **Article URL Discovery Second**: Using the category URLs, the agent discovers all article URLs and stores them in the sitemaps table
3. **Article Extraction Last**: The Content Extraction Agent processes the discovered article URLs, extracting content and metadata

This approach ensures comprehensive coverage while maintaining efficiency and tracking progress throughout the process.

## 1. URL Discovery Process

The URL discovery process is the first step in the scraping workflow. It identifies article URLs from Nigerian news websites for later processing. This process is now enhanced with AI agent capabilities.

### Implementation

**File:** `src/web_scraper/url_discovery_agent.py`

**Key Components:**

1. **URL Discovery Agent:** AI-powered agent for discovering categories and articles
2. **Category Discovery:** Identifies all category pages on a website
3. **Article URL Discovery:** Discovers article URLs from category pages
4. **Sitemaps Storage:** Stores discovered article URLs in the sitemaps table

### Technical Implementation

```python
from agents import Agent, function_tool, Runner
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy, KeywordRelevanceScorer
from typing import List, Dict

@function_tool
async def discover_categories(base_url: str) -> List[Dict]:
    """Discover all category URLs from a website."""
    config = CrawlerRunConfig(
        deep_crawl_strategy=BestFirstCrawlingStrategy(
            max_depth=1,
            include_external=False,
            url_scorer=KeywordRelevanceScorer(
                keywords=["category", "section", "topic", "news"],
                weight=0.7
            )
        ),
        stream=True
    )

    categories = []
    async with AsyncWebCrawler() as crawler:
        async for result in await crawler.arun(base_url, config=config):
            # Use LLM to determine if this is a category page
            if is_category_page(result):
                categories.append({
                    "name": extract_category_name(result),
                    "url": result.url
                })

    # Store categories in database
    await store_categories(categories, base_url)

    return categories

@function_tool
async def discover_article_urls(category_url: str) -> List[str]:
    """Discover article URLs from a category page and store in sitemaps table."""
    config = CrawlerRunConfig(
        deep_crawl_strategy=BestFirstCrawlingStrategy(
            max_depth=2,
            include_external=False,
            url_scorer=KeywordRelevanceScorer(
                keywords=["article", "story", "news", "post"],
                weight=0.8
            )
        ),
        stream=True
    )

    article_urls = []
    async with AsyncWebCrawler() as crawler:
        async for result in await crawler.arun(category_url, config=config):
            # Use LLM to determine if this is an article page
            if is_article_page(result):
                article_urls.append(result.url)

    # Store article URLs in sitemaps table
    await store_article_urls(article_urls, category_url)

    return article_urls
```

### Agent Creation and Execution

We create and execute the URL Discovery Agent using the OpenAI Agents SDK:

```python
# Create the URL Discovery Agent
url_discovery_agent = Agent(
    name="URL Discovery Agent",
    instructions="""You are an expert at discovering URLs from Nigerian news websites.
    First, discover all category URLs from the base URL.
    Then, for each category, discover all article URLs.
    Store all discovered URLs in the database.
    Track your progress so you can resume if interrupted.""",
    model="gpt-4o-mini",
    tools=[discover_categories, discover_article_urls]
)

async def run_url_discovery(base_url: str):
    """Run the URL discovery agent on a website."""
    result = await Runner.run(url_discovery_agent, f"Discover all categories and articles from {base_url}")
    return result
```

### Multi-Site Concurrent Scraping Implementation

We implement concurrent scraping of multiple websites using Crawl4AI's `MemoryAdaptiveDispatcher` and agent orchestration:

```python
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, MemoryAdaptiveDispatcher
from crawl4ai.async_dispatcher import RateLimiter, CrawlerMonitor, DisplayMode
from typing import List, Dict

async def scrape_multiple_websites(website_urls: List[str]):
    """Scrape multiple websites concurrently."""
    # Configure memory-adaptive dispatcher for resource-aware concurrency
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,  # Pause if memory exceeds 70%
        check_interval=1.0,            # Check memory every second
        max_session_permit=5,          # Maximum concurrent websites
        rate_limiter=RateLimiter(
            base_delay=(1.0, 2.0),     # Random delay between requests
            max_delay=30.0,            # Maximum backoff delay
            max_retries=3              # Maximum retry attempts
        ),
        monitor=CrawlerMonitor(
            max_visible_rows=15,
            display_mode=DisplayMode.DETAILED
        )
    )

    # Create website-specific agent instances
    website_agents = {}
    for url in website_urls:
        website_id = get_website_id(url)  # Get or create website ID in database
        website_agents[url] = create_agents_for_website(website_id)

    # Process websites concurrently
    async with AsyncWebCrawler() as crawler:
        # Use streaming mode to process results as they become available
        config = CrawlerRunConfig(stream=True)

        async for result in await crawler.arun_many(
            urls=website_urls,
            config=config,
            dispatcher=dispatcher
        ):
            if result.success:
                # Get the website-specific agents
                agents = website_agents[result.url]

                # Process the website with its dedicated agents
                await process_website_with_agents(result.url, agents)
            else:
                print(f"Failed to process {result.url}: {result.error_message}")
                # Log the failure and schedule retry if needed
                await log_scraping_failure(result.url, result.error_message)

async def process_website_with_agents(website_url: str, agents: Dict):
    """Process a website using its dedicated agents."""
    # Run URL Discovery Agent
    discovery_result = await Runner.run(
        agents["url_discovery"],
        f"Discover all categories and articles from {website_url}"
    )

    # Get discovered category URLs
    category_urls = await get_discovered_categories(website_url)

    # Process each category to discover article URLs
    for category_url in category_urls:
        article_urls_result = await Runner.run(
            agents["url_discovery"],
            f"Discover all article URLs from category {category_url}"
        )

        # Get discovered article URLs
        article_urls = await get_discovered_article_urls(category_url)

        # Process each article URL to extract content
        for article_url in article_urls:
            content_result = await Runner.run(
                agents["content_extraction"],
                f"Extract content from {article_url}"
            )

            # Enhance metadata for the extracted article
            article_id = await get_article_id_from_url(article_url)
            if article_id:
                metadata_result = await Runner.run(
                    agents["metadata_enhancement"],
                    f"Enhance metadata for article {article_id}"
                )
```

### Crawl4AI Integration

We use Crawl4AI's `AsyncWebCrawler` for URL discovery, which provides:

1. **JavaScript Rendering:** Handles dynamic content using Playwright
2. **Asynchronous Processing:** Improves performance with async/await
3. **Configurable Crawling:** Customizable depth, rate limiting, etc.
4. **Streaming Results:** Process results as they become available
5. **Best-First Crawling:** Prioritize high-value pages using relevance scoring

### URL Validation and Storage

1. **Validation:** URLs are validated to ensure they point to actual articles
   - Check URL patterns (e.g., `/news/`, `/article/`, etc.)
   - Filter out non-article pages (e.g., category pages, tag pages)
   - Verify URL accessibility

2. **Storage:** Valid URLs are stored in the database for later processing
   - Store in `sitemaps` table with status "pending"
   - Track discovery method and timestamp

## 2. Article Extraction Process

The article extraction process retrieves and processes content from discovered URLs. This process is now enhanced with AI agent capabilities.

### Implementation

**File:** `src/web_scraper/content_extraction_agent.py`

**Key Components:**

1. **Content Extraction Agent:** AI-powered agent for extracting article content
2. **LLM Extraction Strategy:** Uses language models to extract structured content
3. **Content Cleaning:** Removes ads, navigation, and other non-article content
4. **Metadata Enhancement:** Enriches article metadata through semantic understanding

### Technical Implementation

```python
from agents import Agent, function_tool, Runner
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel
from typing import List, Dict, Optional, Any

class ArticleSchema(BaseModel):
    """Schema for article extraction."""
    title: str
    author: Optional[str] = None
    published_date: Optional[str] = None
    content: str
    categories: List[Dict[str, str]] = []
    tags: List[str] = []
    image_url: Optional[str] = None

@function_tool
async def extract_article_content(url: str, website_id: int) -> Dict[str, Any]:
    """Extract structured content from an article URL using LLM extraction."""
    # Configure LLM extraction strategy
    extraction_strategy = LLMExtractionStrategy(
        model="gpt-4o",
        schema=ArticleSchema.schema(),
        instructions="Extract the article content, removing ads, navigation, and social media elements."
    )

    config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        word_count_threshold=10,
        excluded_tags=['nav', 'footer', 'aside', 'form'],
        remove_overlay_elements=True
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url, config=config)

        if not result.success:
            return {"error": result.error_message}

        # Extract structured content using LLM
        article_data = result.extracted_content

        # Calculate additional metadata
        word_count = calculate_word_count(article_data["content"])
        reading_time = calculate_reading_time(word_count)

        # Enhance the article data
        enhanced_data = {
            "title": article_data["title"],
            "url": url,
            "content": article_data["content"],
            "author": article_data.get("author"),
            "published_date": article_data.get("published_date"),
            "image_url": article_data.get("image_url"),
            "website_id": website_id,
            "article_metadata": {
                "word_count": word_count,
                "reading_time": reading_time,
                "categories": article_data.get("categories", []),
                "tags": article_data.get("tags", []),
                "extraction_method": "llm"
            }
        }

        # Store the article in the database
        await store_article(enhanced_data)

        return enhanced_data
```

### Agent Creation and Execution

We create and execute the Content Extraction Agent using the OpenAI Agents SDK:

```python
# Create the Content Extraction Agent
content_extraction_agent = Agent(
    name="Content Extraction Agent",
    instructions="""You are an expert at extracting content from Nigerian news articles.
    Extract the title, author, publication date, content, categories, and tags.
    Clean the content by removing ads, navigation, social media buttons, etc.
    Store the extracted content in the database.""",
    model="gpt-4o",
    tools=[extract_article_content]
)

async def run_content_extraction(article_urls: List[str], website_id: int):
    """Run the content extraction agent on a list of article URLs."""
    results = []
    for url in article_urls:
        result = await Runner.run(
            content_extraction_agent,
            f"Extract content from {url} for website ID {website_id}"
        )
        results.append(result)
    return results
```

### Extraction Strategies

We implement multiple extraction strategies to handle different website structures:

1. **LLM-based Extraction:** Primary strategy using language models for content extraction
   - Implemented with `LLMExtractionStrategy`
   - Robust for unknown or complex website structures
   - Adapts to different website layouts without manual configuration

2. **CSS-based Extraction:** Fallback strategy for known website structures
   - Implemented with `JsonCssExtractionStrategy`
   - More efficient but requires manual configuration

3. **Fallback Mechanisms:** Implements fallbacks when primary strategies fail
   - Generic extraction based on common patterns
   - Heuristic-based content identification

### Content Processing

1. **HTML Cleaning:** Removes ads, navigation, social media buttons, sharing options, tag sections, and other non-article content
2. **Markdown Conversion:** Converts HTML to Markdown for better processing and LLM training
3. **Image Handling:** Extracts and processes article images
4. **Date Normalization:** Standardizes date formats
5. **Content Boundary Detection:** Identifies the start and end of the actual article content

## 3. Metadata Enhancement

Metadata enhancement is a critical part of the workflow, ensuring articles are properly categorized and enriched for research and analysis. This process is now enhanced with AI agent capabilities.

### Implementation

**File:** `src/web_scraper/metadata_enhancement_agent.py`

**Key Components:**

1. **Metadata Enhancement Agent:** AI-powered agent for enriching article metadata
2. **Category Analysis:** Analyzes and enhances article categories
3. **Tag Generation:** Generates relevant tags based on article content
4. **Vectorization Preparation:** Prepares content for vectorization

### Technical Implementation

```python
from agents import Agent, function_tool, Runner
from pydantic import BaseModel
from typing import List, Dict, Optional, Any

class EnhancedMetadata(BaseModel):
    """Schema for enhanced article metadata."""
    categories: List[Dict[str, str]]
    tags: List[str]
    summary: str
    keywords: List[str]
    sentiment: Optional[str] = None
    topics: List[str] = []
    vectorization_ready: bool = True

@function_tool
async def enhance_article_metadata(article_id: int) -> Dict[str, Any]:
    """Enhance article metadata for better categorization and vectorization."""
    # Retrieve article from database
    article = await get_article_by_id(article_id)

    if not article:
        return {"error": "Article not found"}

    # Extract existing metadata
    existing_metadata = article.get("article_metadata", {})

    # Analyze content to generate enhanced metadata
    content = article.get("content", "")
    title = article.get("title", "")

    # Generate tags if not present or insufficient
    tags = existing_metadata.get("tags", [])
    if len(tags) < 5:
        tags = generate_tags(content, title)

    # Enhance categories with additional context
    categories = existing_metadata.get("categories", [])
    enhanced_categories = enhance_categories(categories, content)

    # Generate summary for vectorization
    summary = generate_summary(content)

    # Extract keywords for improved search
    keywords = extract_keywords(content, title)

    # Identify main topics
    topics = identify_topics(content)

    # Create enhanced metadata
    enhanced_metadata = {
        "categories": enhanced_categories,
        "tags": tags,
        "summary": summary,
        "keywords": keywords,
        "topics": topics,
        "vectorization_ready": True
    }

    # Update article in database
    await update_article_metadata(article_id, enhanced_metadata)

    return enhanced_metadata
```

### Agent Creation and Execution

We create and execute the Metadata Enhancement Agent using the OpenAI Agents SDK:

```python
# Create the Metadata Enhancement Agent
metadata_enhancement_agent = Agent(
    name="Metadata Enhancement Agent",
    instructions="""You are an expert at enhancing metadata for Nigerian news articles.
    Analyze the article content to extract additional metadata.
    Enhance categorization and tagging.
    Prepare the content for vectorization.""",
    model="gpt-4o-mini",
    tools=[enhance_article_metadata]
)

async def run_metadata_enhancement(article_ids: List[int]):
    """Run the metadata enhancement agent on a list of article IDs."""
    results = []
    for article_id in article_ids:
        result = await Runner.run(
            metadata_enhancement_agent,
            f"Enhance metadata for article {article_id}"
        )
        results.append(result)
    return results
```

### Website-Specific Categories

Categories are website-specific, meaning each website has its own set of categories:

1. **Database Schema:** Categories table includes `website_id` foreign key
2. **Category Creation:** New categories are created with the website's ID
3. **Category Lookup:** Categories are looked up by name and website ID
4. **Category Enhancement:** The Metadata Enhancement Agent enriches categories with additional context

This approach ensures that categories from different websites don't conflict, even if they have the same name, while still providing rich metadata for vectorization.

## 4. Data Storage

The final step in the workflow is storing the extracted and processed data in the database.

### Implementation

**File:** `src/service_layer/article_service.py`

**Key Components:**

1. **Database Models:** ORM models for articles, websites, categories, etc.
2. **Repository Layer:** Data access methods for each entity
3. **Service Layer:** Business logic for storing and retrieving data

### Technical Implementation

```python
# Simplified implementation
def extract_and_store_article(self, url: str, website_id: int) -> Optional[Article]:
    """Extract and store an article from a URL."""
    # Extract article data
    article_data = self.article_extractor.extract_article(url, website_id)

    if not article_data:
        return None

    # Create article in database
    article = self.article_repo.create_article(article_data)

    # Add categories if available
    categories = article_data.get("article_metadata", {}).get("categories", [])
    category_urls = article_data.get("article_metadata", {}).get("category_urls", [])

    for i, category_name in enumerate(categories):
        category_url = category_urls[i] if i < len(category_urls) else None

        # Try to find existing category
        category = self.website_repo.get_category_by_name(website_id, category_name)

        if not category:
            # Create new category
            category = self.website_repo.create_category(website_id, {
                "name": category_name,
                "url": category_url or f"{base_url}/category/{category_name.lower().replace(' ', '-')}"
            })

        # Add category to article
        self.article_repo.add_article_category(article.id, category.id)

    return article
```

### Database Schema

The database schema includes the following tables:

1. **websites:** Stores information about news websites
2. **articles:** Stores article content and metadata
3. **categories:** Stores category information (website-specific)
4. **article_categories:** Junction table linking articles to categories
5. **scraping_jobs:** Tracks scraping job status and progress
6. **scraping_errors:** Logs errors encountered during scraping

### Data Optimization

1. **Compression:** Compresses article content to reduce storage requirements
2. **Indexing:** Creates appropriate indexes for efficient querying
3. **Archiving:** Implements data retention policies for older content

## Challenges and Solutions

### 1. Inconsistent Website Structures

**Challenge:** Nigerian news websites have varying structures, URL formats, and content organization.

**Solution:**
- Implement multiple discovery methods (sitemap, RSS, category pages)
- Use adaptive extraction strategies based on website patterns
- Implement website-specific configurations when necessary

### 2. Dynamic Content and JavaScript

**Challenge:** Many websites use JavaScript to load content dynamically.

**Solution:**
- Use Crawl4AI with Playwright for browser automation
- Configure appropriate wait times for content loading
- Implement content verification to ensure complete extraction

### 3. Anti-Scraping Measures

**Challenge:** Websites implement various anti-scraping measures.

**Solution:**
- Implement rate limiting and request throttling
- Use proxy rotation for high-volume scraping
- Respect robots.txt and implement polite scraping practices
- Use realistic user agents and browser fingerprints

### 4. Complex Data Relationships

**Challenge:** Maintaining proper relationships between websites, articles, and categories.

**Solution:**
- Implement robust database schema with appropriate foreign keys
- Use transaction management to ensure data consistency
- Implement validation before database operations
- Create utility scripts to fix data inconsistencies

### 5. Error Handling and Resilience

**Challenge:** Many potential points of failure in the scraping process.

**Solution:**
- Implement comprehensive error logging
- Use retry mechanisms with exponential backoff
- Implement circuit breakers for failing endpoints
- Create monitoring and alerting for critical failures

### 6. Resource Constraints

**Challenge:** Limited resources on standard VPS hosting.

**Solution:**
- Implement scheduled batch processing instead of continuous scraping
- Use intelligent crawling to only process changed content
- Implement content compression to reduce storage requirements
- Separate compute and storage concerns for flexibility

## Vectorization Preparation

To prepare the scraped data for vectorization and LLM training:

1. **Content Quality:** Ensure high-quality, clean content without ads or irrelevant elements
2. **Metadata Enrichment:** Include rich metadata for context (categories, publication date, source)
3. **Structured Format:** Store content in a consistent, structured format
4. **Text Normalization:** Standardize text formatting, encoding, and special characters
5. **Language Detection:** Identify and tag the language of each article
6. **Content Deduplication:** Identify and handle duplicate or near-duplicate content

## Monitoring and Management

The dashboard provides tools for monitoring and managing the scraping process:

1. **Job Management:** Start, stop, and monitor scraping jobs
2. **Error Tracking:** View and analyze scraping errors
3. **Performance Metrics:** Track scraping performance and resource usage
4. **Content Statistics:** View statistics on scraped content
5. **URL Management:** Add and manage URLs for scraping

## Conclusion

The Naija News Hub scraping workflow is designed to handle the complexities of Nigerian news websites while efficiently collecting and processing content for research and analysis. By implementing multiple discovery methods, adaptive extraction strategies, and robust error handling, the system can reliably scrape content from diverse sources.

The architecture prioritizes data quality, efficiency, and resilience, ensuring that the collected data is suitable for vectorization and LLM training. Ongoing improvements focus on enhancing extraction accuracy, optimizing resource usage, and improving monitoring capabilities.

## Ongoing Maintenance Strategy

After the initial scraping of websites, we implement an efficient maintenance strategy to keep the content up-to-date while minimizing resource usage and API costs.

### Incremental Crawling

1. **Change Detection**:
   - Store checksums or hashes of previously crawled category pages
   - Compare current page hash with stored hash to detect changes
   - Only process pages that have changed since the last crawl

2. **Optimized Crawling Frequency**:
   - **High-Activity Sites** (multiple updates per day): Every 6 hours
   - **Medium-Activity Sites** (daily updates): Once per day
   - **Low-Activity Sites** (weekly updates): Twice per week
   - Dynamically adjust based on observed update patterns

3. **Resource-Efficient Implementation**:
   - Use lightweight HEAD requests to check for changes before full crawling
   - Implement exponential backoff for sites with no changes
   - Schedule crawls during off-peak hours (e.g., midnight to 5 AM)

### Implementation

```python
async def check_for_updates(website_id: int):
    """Check if a website has new content since the last crawl."""
    website = await get_website_by_id(website_id)
    categories = await get_categories_by_website_id(website_id)

    updates_found = False

    for category in categories:
        # Get stored hash and last check time
        stored_hash = category.get("page_hash")
        last_checked = category.get("last_checked_at")

        # Check if we need to crawl based on the site's update frequency
        if not should_check_category(category, last_checked):
            continue

        # Make a lightweight HEAD request to check Last-Modified header
        headers = await make_head_request(category["url"])
        if not has_changed(headers, last_checked):
            # Update last checked time but don't crawl
            await update_category_check_time(category["id"])
            continue

        # Get the current page and calculate its hash
        page_content = await fetch_category_page(category["url"])
        current_hash = calculate_hash(page_content)

        # Compare with stored hash
        if current_hash != stored_hash:
            # Page has changed, discover new articles
            new_articles = await discover_new_articles(category, page_content)
            if new_articles:
                updates_found = True
                # Process new articles
                await process_new_articles(new_articles, website_id)

            # Update the stored hash and last checked time
            await update_category_hash(category["id"], current_hash)

    return updates_found

async def process_new_articles(article_urls: List[str], website_id: int):
    """Process newly discovered articles."""
    # Extract content using Content Extraction Agent
    for url in article_urls:
        article_data = await extract_article_content(url, website_id)

        # Store article in database
        article_id = await store_article(article_data)

        # Enhance metadata using Metadata Enhancement Agent
        enhanced_metadata = await enhance_article_metadata(article_id)

        # Vectorize article for LLM training
        await vectorize_article(article_id)
```

### Vectorization Integration

Newly discovered articles are automatically vectorized and added to the vector database for LLM training:

1. **Vectorization Process**:
   - Extract key content from the article
   - Generate embeddings using appropriate model
   - Store embeddings in vector database

2. **LLM Integration**:
   - Periodically update LLM training data with new vectors
   - Implement incremental training to incorporate new content
   - Track performance metrics to ensure quality

## Related Files

### Agent Implementation Files
- `src/web_scraper/url_discovery_agent.py`: URL Discovery Agent implementation
- `src/web_scraper/content_extraction_agent.py`: Content Extraction Agent implementation
- `src/web_scraper/metadata_enhancement_agent.py`: Metadata Enhancement Agent implementation
- `src/web_scraper/maintenance_agent.py`: Maintenance Agent implementation

### Core Implementation Files
- `src/service_layer/article_service.py`: Article service implementation
- `src/database_management/models.py`: Database models
- `src/database_management/repositories/`: Repository implementations
- `src/vectorization/article_vectorizer.py`: Article vectorization implementation

### Documentation Files
- `docs/dev/crawl4ai-integration.md`: Crawl4AI integration documentation
- `docs/dev/efficient-scraping-architecture.md`: Scraping architecture documentation
- `docs/dev/database-schema.md`: Database schema documentation
- `docs/dev/crawl4ai-with-agent-transcript.md`: Crawl4AI with agent integration documentation
- `docs/dev/maintenance-strategy.md`: Maintenance strategy documentation
