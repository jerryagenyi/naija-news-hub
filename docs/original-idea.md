# Original Description:

I need a PRD; I am initiating a Nigerian news scraping project in python. The thing is most news websites in nigeria may not be as accessible as expected. Varying post/article url formats, varying sitemap indexing formats, some sitemaps not accessible, etc. Infact some of the websites list their articles dynamically rather than with pagination. I have atleast 28 websites to scrape news from and wish to develop a project that wil take a URL as the base url and just scrape all articles.The plan is on the frontend (maybe nextjs page), you are presented with a simple form to input a URL. It should be a multi-step form wher eyou input the base url, it runs some check and if successful, it will ask you for a next step, etc.

The plan for the flow is to first generate a sitemap of all available article URLS of the website, then another script to use those urls to parse the pages and extract the articles (title, author, publication date, category/categories, article content) and store in another table.

I believe we would have a schema that has the following tables:

\- websites: it will store the base urls for each websites.

\- sitemaps: to store article urls scraped from the websites each

\- categories: to store categories of articles from the websites each

\- articles: stores scraped articles based on the sitemaps table

when a base urls is recieved, a script checks if there is a sitemap index for that base url based on a function we will run. If it is available and it returns 200 status code, we store the index in the websites table. If accessible, we check for the range of the index. we check the starting page of the archive index (like some sitemap index start with say post-sitemap.xml as the first item, while others might be post-sitemap-1.xml \- you get the idea, as there are many formats and is at the discretion of the organisation owning the website). So we store the first and last archive url along with the index page url. This is so that we can write another function to take the last archive url and check the last entry if it has changed for new pages (I am sure you get the idea).

If the above is successful, the first step of the multi-stage form is complete. If all status returns 200, it means the base url is accessible, it has a sitemap index, and the index is accessible. the based on this you will be presented with a button to scrape the URLs. once its complete, you will be done with this aspect of the project.

so to be clear, this aspect of the project is one of the options a user will be able to use. it will be called something like "Sitemap Generator". Other options will be in the future aspect of the project as shown below. once the URLs have been generated successfully into the sitemap table, this stage is complete \- we should have a way of noting that sitemap has been generated for the website. and a way of loggin errors in maybe another table with a status if it has been resolved or not. And a way to know it has been resolved will be that the URL is tested and returns a 200 code. The sitemaps table has a function to check the website if there are new articles so it can be updated. Remember all article urls must be tested to return 200 before they are stored, or a column to say what the status is \- maybe a script that will run once every month through out the month to check for changes in currently stored urls; this is in addition to the script that checks for new articles based on the publication date of the last stored url.

Future aspects will include:

1. Once the URLs have been generated, another aspect will be to scrape the articles. IT will need a form where you will be asked to input css tags of various elements we seek. Future features of this will be to us an LLM to parse the website, fetch an article or news item, parse the page and intelligently determine from the content what the various elements are for the title, author, etc, and feed that back into our form. Then these elements are stored in another table or along with the websites table (if that wont complicate the table). Then based on this, we can now scrape the article and update the articles table with a success or failure status, and also show a progress status bar by measuring the content of articles table for a particular website vs how many urls is in the sitemaps table for that website.  
2. For websites without a sitemap index or inaccessible sitemap index, we need a different approach for generating the article urls. Since there is not sitemap to fetch the URLs and store in the sitemaps table, we would need a way to retrieve the categories and their URLs on the website, store those in the categories table (and be sure the status is 200 for each), and then use those urls to check how articles are archived on the page, then use a logic to check how the pagination url is, then use that to check the range of the archive pages. These information are then stored in the websites table (or whatever table we create to store such information \- along with css selectors of the elements we are interested in, etc. Based on these we can now fetch and store all URLS of all articles ever published on the website (perhaps we should have a way to store the progress of this, so that if the process breaks and needs to start again, it can always know where it stopped last, what has been scraped and what has not been scraped. Same also for when we scrape articles. One way could be if we hit any error fetching any url, it should be logged somewhere so we can always know what has been scraped and what has not been scraped). Then based on these we can now scrape the articles themselves.  
3. Third aspect is to now pull this together so various n8n AI agents can use the various logic to maintain the project: agent to check for new articles across the entire 28+ websites, always, another to track errors, etc. Or maybe a single agent can handle it \- who knows. Then we automate this with n8n and let the project live. 

# The information is vectorised and stored for LLM to use. Based on all of the above, we develop an LLM that can interact with the vectorise information to allow researchers to chat with LLM for the content. For research purposes, etc. 